# graduation-project
ðŸ¤—CQUè®¡ç®—æœºå­¦é™¢æ¯•ä¸šè®¾è®¡â€”â€”åŸºäºŽMindSporeçš„Falconå¤§æ¨¡åž‹è¿ç§»ä¸Žæ€§èƒ½ç ”ç©¶

é¡¹ç›®éƒ¨åˆ†ä¸ºåŽä¸ºæ˜‡æ€MindSporeç¤¾åŒºå¼€æºä»»åŠ¡ï¼ŒåŸºäºŽMindSporeçš„ä¸€ä¸ªNLPæ˜“ç”¨å¼€æºåº“MindNLP https://github.com/mindlab-ai/mindnlp

srcç›®å½•ä¸ºfalconçš„MindSporeå®žçŽ°ï¼Œå®žéªŒè¯æ˜Žè¯¥ç‰ˆæœ¬å®žçŽ°ä¸ŽHuggingFaceä¸­æ¨¡åž‹å®žçŽ°åœ¨å®¹çº³è¯¯å·®ä¸º$`10^{-3}`$çš„å‰æä¸‹ä¸¤è€…ç­‰æ•ˆï¼Œå®Œæ•´æ¨¡åž‹å·²ä¸Šä¼ è‡³modelscopeç¤¾åŒº https://www.modelscope.cn/models/mindnlp/falcon-rw-1b/summary ï¼ŒåŒ…æ‹¬æ¨¡åž‹å®žçŽ°ã€æ‰€éœ€é…ç½®æ–‡ä»¶ä»¥åŠè½¬æ¢å¥½çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ã€‚
å•å…ƒæµ‹è¯•ä»£ç è§testç›®å½•ã€‚

train_falconä¸ºåœ¨MindNLPæ¡†æž¶ä¸­åŸºäºŽfalcon-rw-1bé¢„è®­ç»ƒæ¨¡åž‹è¿›è¡Œå¾®è°ƒçš„ä»£ç ï¼Œæ•°æ®é›†ä¸ºGLUEåŸºå‡†æ•°æ®é›†ä¸­çš„MRPCè¯­æ–™ï¼Œä»»åŠ¡æ˜¯è¯­ä¹‰åŒ¹é…ã€‚

FlashAttention kernelç”¨CUDAç¼–å†™ï¼Œå‚è€ƒäº†ä»£ç https://github.com/tspeterkim/flash-attention-minimal/blob/main/flash.cu ï¼Œç›®å‰ä¸ºåªæ”¯æŒé™æ€block sizeä¸ŽFP32æ•°æ®æ ¼å¼

## çŽ¯å¢ƒæ­å»º
```bash
#å®‰è£…Mindnlp
pip install git+https://github.com/mindspore-lab/mindnlp.git
```

## requirement
```bash
pip install "mindspore>=2.2"
```

## quickly start
```python
repo = "Rocketknight1/falcon-rw-1b"
tokenizer = AutoTokenizer.from_pretrained(repo)
model = AutoModelForCausalLM.from_pretrained(repo)
model.set_train(False)
pipeline = mindnlp.transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=ms.bfloat16,
    trust_remote_code=True,
)

sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")

```
