# graduation-project

ðŸ¤—CQUè®¡ç®—æœºå­¦é™¢æ¯•ä¸šè®¾è®¡â€”â€”åŸºäºŽMindSporeçš„Falconå¤§æ¨¡åž‹è¿ç§»ä¸Žæ€§èƒ½ç ”ç©¶ã€‚

é¡¹ç›®éƒ¨åˆ†ä¸ºåŽä¸ºæ˜‡æ€MindSporeç¤¾åŒºå¼€æºä»»åŠ¡ï¼ŒåŸºäºŽMindSporeçš„ä¸€ä¸ªNLPæ˜“ç”¨å¼€æºåº“MindNLP https://github.com/mindlab-ai/mindnlpã€‚

## æ¨¡åž‹è¿ç§»ä¸Žç²¾åº¦å¯¹é½

srcç›®å½•ä¸ºfalconçš„MindSporeå®žçŽ°ï¼Œå®žéªŒè¯æ˜Žè¯¥ç‰ˆæœ¬å®žçŽ°ä¸ŽHuggingFaceä¸­æ¨¡åž‹å®žçŽ°åœ¨å®¹çº³è¯¯å·®ä¸º$`10^{-3}`$çš„å‰æä¸‹ä¸¤è€…ç­‰æ•ˆï¼Œå®Œæ•´æ¨¡åž‹å·²ä¸Šä¼ è‡³modelscopeç¤¾åŒºï¼Œæˆªè‡³ç›®å‰ä¸‹è½½é‡å·²è¾¾**3.1k** ã€‚https://www.modelscope.cn/models/mindnlp/falcon-rw-1b/summary ï¼ŒåŒ…æ‹¬æ¨¡åž‹å®žçŽ°ã€æ‰€éœ€é…ç½®æ–‡ä»¶ä»¥åŠè½¬æ¢å¥½çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ï¼Œä¹Ÿå¯åœ¨[hypertseng/falcon-rw-1b_mindspore (github.com)](https://github.com/hypertseng/falcon-rw-1b_mindspore)é‡ŒèŽ·å–é™¤checkpointä¹‹å¤–çš„ä»£ç ä¸Žé…ç½®æ–‡ä»¶ã€‚
å•å…ƒæµ‹è¯•åŒ…å«æ¨¡åž‹é…ç½®ä¸Žæ–‡æœ¬ç”Ÿæˆçš„67é¡¹æµ‹è¯•ï¼Œå·²å…¨éƒ¨é€šè¿‡ï¼Œå…·ä½“æµ‹è¯•é¡¹ç›®ä¸Žä»£ç è§testç›®å½•ã€‚æ­å»ºå¥½é¡¹ç›®çŽ¯å¢ƒåŽï¼Œè¿›å…¥mindnlpç›®å½•åœ¨ç»ˆç«¯è¾“å…¥ä»¥ä¸‹å‘½ä»¤å¯è¿›è¡Œæµ‹è¯•ï¼š

```
export RUN_SLOW=True  // è‹¥éœ€è¦è¿›è¡Œæ–‡æœ¬ç”Ÿæˆæµ‹è¯•ï¼ˆè€—è´¹æ—¶é—´å¾ˆé•¿ï¼‰
pytest -vs tests/ut/transformers/models/falcon
```

## å•å¡loraå¾®è°ƒ

train_falconä¸ºåœ¨MindNLPæ¡†æž¶ä¸­åŸºäºŽfalcon-rw-1bé¢„è®­ç»ƒæ¨¡åž‹è¿›è¡Œå¾®è°ƒçš„ä»£ç ï¼Œæ•°æ®é›†ä¸ºGLUEåŸºå‡†æ•°æ®é›†ä¸­çš„MRPCè¯­æ–™ï¼Œä»»åŠ¡æ˜¯è¯­ä¹‰åŒ¹é…ã€‚

é»˜è®¤è®­ç»ƒ10epochï¼Œå› ä¸ºæ¨¡åž‹æœ¬èº«ç†è§£èƒ½åŠ›è¾ƒå¼ºï¼Œå®žéªŒè¡¨æ˜Žåœ¨è®­ç»ƒ5ä¸ªepochä¹‹åŽå·²åŸºæœ¬æ”¶æ•›ï¼Œè®­ç»ƒå¥½çš„æ–‡ä»¶å·²æ”¾åœ¨peft_modelç›®å½•ä¸‹ï¼Œå¯ç›´æŽ¥åŠ è½½ã€‚

### æ•°æ®é›†ä¸‹è½½

é€šè¿‡mrpc_dataset.pyè„šæœ¬ä¸­load_examplesæŽ¥å£è‡ªåŠ¨ä»ŽHugging Faceä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†ã€‚

### æ¨¡åž‹ä¸‹è½½

å¯é€šè¿‡Huggingfaceé•œåƒç«™å¿«é€ŸåŠ è½½æ¨¡åž‹ã€‚

```bash
wget https://hf-mirror.com/hfd/hfd.sh
chmod a+x hfd.sh
./hfd.sh Rocketknight1/falcon-rw-1b --tool aria2c -x 4  # è¿™é‡Œé€‰æ‹©ä¸‹è½½è§„æ¨¡æœ€å°çš„ç‰ˆæœ¬ï¼Œè‹¥æœ‰è¶³å¤Ÿçš„ç¡¬ä»¶æ”¯æŒï¼Œå¯ä¸‹è½½å…¶ä»–ç‰ˆæœ¬
```

### è®­ç»ƒå‘½ä»¤

è®­ç»ƒä¹‹å‰å…ˆç¡®ä¿å·²ç»ä¸‹è½½å¥½æ¨¡åž‹æ–‡ä»¶ï¼Œæ–‡ä»¶é»˜è®¤å­˜æ”¾ç›®å½•ä¸º.mindnlp/model/Rocketknight1/falcon-rw-1bã€‚

åœ¨mindnlpæ ¹ç›®å½•ä¸‹æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ã€‚

```
python llm/peft/train_falcon/train_mrpc.py \
--save_dir ".mindnlp/peft_model/falcon/mrpc_lora" \
--batch_size 8 \
--model_name_or_path ".mindnlp/model/Rocketknight1/falcon-rw-1b" \
--max_seq_len 256 \
--lora
```

## FlashAttentionæ”¯æŒ

åŽŸé¢„è®¡é€šè¿‡AOT CompileråŸºäºŽé«˜çº§é¢†åŸŸç‰¹å®šè¯­è¨€(Domain-Specific Languages, DSL)Tritonç¼–å†™çš„è¾ƒå®Œå¤‡çš„FlashAttentionå®žçŽ°ç¼–è¯‘å¾—åˆ°Low Levelçš„CUDA kernelï¼Œå†é€šè¿‡Customè‡ªå®šä¹‰ç®—å­çš„æ–¹å¼åŠ è½½è¿›MindSporeï¼Œä½†ç»ç ”ç©¶å‘çŽ°ï¼ŒTriton AOT Compilerçš„ç¼–è¯‘é€»è¾‘åœ¨é¢„å®šæ¨¡æ¿ä»£ç çš„åŸºç¡€ä¸Šç›´æŽ¥åµŒå…¥äº†é€šè¿‡PTXç”Ÿæˆçš„cubinäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œcubinæ–‡ä»¶åªèƒ½åœ¨CUDA Runtimeä¸­åŠ è½½è¿è¡Œï¼Œå¹¶ä¸æ˜¯ç”Ÿæˆkernelä»£ç ã€‚å› æ­¤ï¼Œå‚è€ƒäº†ä»£ç https://github.com/tspeterkim/flash-attention-minimal/blob/main/flash.cu ï¼Œç”¨çº¯CUDA C++ç¼–å†™äº†FlashAttention kernelï¼Œç›®å‰å®žçŽ°äº†FlashAttention_v1ä¸ŽFlashAttention_v2çš„æ­£åå‘è®¡ç®—ï¼ˆå«causal maskï¼‰ï¼Œä½†åªæ”¯æŒé™æ€block sizeä¸ŽFP32æ•°æ®æ ¼å¼ï¼Œå¯å®žçŽ°**3.5x-94x**çš„åŠ é€Ÿã€‚ä»“åº“ä¸­åŒ…å«äº†è°ƒè¯•FlashAttention_v1ä¸ŽFlashAttention_v2çš„çš„vscodeè°ƒè¯•è®¾ç½®ä¸ŽCUDA C++ä»£ç ï¼Œå¯ä¾›åˆå­¦è€…ä½¿ç”¨ã€‚

å¯åœ¨mindnlpæ ¹ç›®å½•ä¸‹é€šè¿‡ä»¥ä¸‹è„šæœ¬æµ‹è¯•å…¶æ­£ç¡®æ€§å¹¶é€šè¿‡mindsightæŸ¥çœ‹æ€§èƒ½åˆ†æžç»“æžœï¼š

```bash
# å¼€å¯mindinsight è®°å½•ç®—å­è¿è¡Œè€—æ—¶
mindinsight start

pytest -vs ./tests/ut/modules/test_flashattention.py
```

FlashAttentionè®ºæ–‡å‡ºå¤„å¦‚ä¸‹ï¼š

![1712827717331](image/README/1712827717331.png)

**FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©
Paper: https://arxiv.org/abs/2205.14135

**FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning**
Tri Dao

Paper: https://tridao.me/publications/flash2/flash2.pdf

## Create your ChatBot in 5 min

### çŽ¯å¢ƒæ­å»º

```bash
pip install git+https://github.com/mindspore-lab/mindnlp.git
# or
git clone https://github.com/mindspore-lab/mindnlp.git
cd mindnlp
bash scripts/build_and_reinstall.sh#å®‰è£…Mindnlp
pip install git+https://github.com/mindspore-lab/mindnlp.git
```

### requirements

```bash
pip install "mindspore>=2.2"
```

### quickly start

```python
repo = "Rocketknight1/falcon-rw-1b"
tokenizer = AutoTokenizer.from_pretrained(repo)
model = AutoModelForCausalLM.from_pretrained(repo)
model.set_train(False)
pipeline = mindnlp.transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=ms.bfloat16,
    trust_remote_code=True,
)

sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")

```
